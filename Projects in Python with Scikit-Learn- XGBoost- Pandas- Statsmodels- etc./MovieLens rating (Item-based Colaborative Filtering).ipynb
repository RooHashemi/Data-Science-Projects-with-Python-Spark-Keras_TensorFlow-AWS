{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description & Problem statement: \n",
    "One of the most common datasets that is available on the internet for building a Recommender System is the MovieLens DataSet. This version of the dataset that I'm working with (100K) contains 100,000 ratings from 1000 users on 1700 movies. Released 4/1998. The data was collected by GroupLens researchers over various periods of time, depending on the size of the set. Users were selected at random for inclusion. All users selected had rated at least 20 movies. Each user is represented by an id, and no other information is provided. The original data are contained in three files, movies.dat, ratings.dat and users.dat. \n",
    "In this project, I use the Item-based collaborative filtering to predict the rating of a user on new item.\n",
    "\n",
    "\n",
    "# Theory of Collaborative Filtering (CF):\n",
    "There are 2 main types of memory-based collaborative filtering algorithms:\n",
    "\n",
    "1) User-User Collaborative Filtering: Here I find look alike users based on similarity and recommend movies which first user’s look-alike has chosen in past. This algorithm is very effective but takes a lot of time and resources. It requires to compute every user pair information which takes time. Therefore, for big base platforms, this algorithm is hard to implement without a very strong parallelizable system.\n",
    "\n",
    "2) Item-Item Collaborative Filtering: It is quite similar to previous algorithm, but instead of finding user's look-alike, I try finding movie's look-alike. Once we have movie's look-alike matrix, we can easily recommend alike movies to user who have rated any movie from the dataset. This algorithm is far less resource consuming than user-user collaborative filtering. Hence, for a new user, the algorithm takes far lesser time than user-user collaborate as I don’t need all similarity scores between users. And with fixed number of movies, movie-movie look alike matrix is fixed over time.\n",
    "\n",
    "Note: Here, I use Item-based CF.\n",
    "\n",
    "# Workflow:\n",
    "- Load and merge the datasets\n",
    "- Creat functions to calculate the Item-Item similarity, using:\n",
    "     - Euclidian distance\n",
    "     - Pearson's correlation\n",
    "     - Corrected Pearson's correlation\n",
    "     \n",
    "- Object Oriented Programming to:\n",
    "     - compute the Item-Item similarity matrix for all items, using one of similarity functions defined above,\n",
    "     - predict the User rating on a new Item, using all Item-Item similarities. \n",
    "\n",
    "- Define functions to calculate RMSE and evaluate the Recommender System\n",
    "- Evaluate the Recommender System for different similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from math import isnan\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='times')\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plt.rc('font', size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 3EA9-93A4\n",
      "\n",
      " Directory of C:\\Users\\rhash\\Documents\\Datasets\\Recommender systems\n",
      "\n",
      "09/15/2018  12:13 AM    <DIR>          .\n",
      "09/15/2018  12:13 AM    <DIR>          ..\n",
      "09/14/2018  08:05 PM    <DIR>          .ipynb_checkpoints\n",
      "09/15/2018  12:13 AM         2,145,491 itemBased_CF (Movie Lens Data).ipynb\n",
      "09/14/2018  09:42 AM    <DIR>          ml-100k\n",
      "09/15/2018  12:11 AM            45,651 userBased_CF (Movie Lens Data).ipynb\n",
      "               2 File(s)      2,191,142 bytes\n",
      "               4 Dir(s)  390,184,722,432 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La BD has 100000 ratings\n",
      "La BD has  943  users\n",
      "La BD has  1682  movies\n",
      "\n",
      "   user_id         title  movie_id  rating release_date sex  age\n",
      "0      196  Kolya (1996)       242       3  24-Jan-1997   M   49\n",
      "1      305  Kolya (1996)       242       5  24-Jan-1997   M   23\n",
      "2        6  Kolya (1996)       242       4  24-Jan-1997   M   42\n",
      "3      234  Kolya (1996)       242       4  24-Jan-1997   M   60\n",
      "4       63  Kolya (1996)       242       3  24-Jan-1997   M   31\n"
     ]
    }
   ],
   "source": [
    "# Load Data set\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols)\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=r_cols)\n",
    "\n",
    "# movies file contains columns indicating the movie's genres\n",
    "# let's load only the first three columns of movie file with usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date']\n",
    "movies = pd.read_csv('ml-100k/u.item', sep='|', names=m_cols, usecols=range(3), encoding='cp1250')\n",
    "\n",
    "# merge the DataFrame\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "data = data[['user_id','title', 'movie_id','rating','release_date','sex','age']]\n",
    "\n",
    "\n",
    "print(\"La BD has \"+ str(data.shape[0]) +\" ratings\")\n",
    "print(\"La BD has \", data.user_id.nunique(),\" users\")\n",
    "print(\"La BD has \", data.movie_id.nunique(), \" movies\\n\")\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of common users: 4 \n",
      "\n",
      "       user_id  rating\n",
      "95661      181       1\n",
      "95662       90       4\n",
      "95665        1       5\n",
      "95676      655       4 \n",
      "\n",
      "       user_id  rating\n",
      "96837      181       1\n",
      "96839       90       3\n",
      "96840        1       4\n",
      "96846      655       3\n"
     ]
    }
   ],
   "source": [
    "# dataframe with the data from user 1\n",
    "data_movie_1 = data[data.movie_id==6]\n",
    "\n",
    "# dataframe with the data from user 2\n",
    "data_movie_2 = data[data.movie_id==18]\n",
    "\n",
    "# We first compute the set of common movies\n",
    "common_users = set(data_movie_1.user_id) & set(data_movie_2.user_id)\n",
    "print(\"\\nNumber of common users:\", len(common_users),'\\n')\n",
    "\n",
    "# creat the subdataframe with only with the common movies\n",
    "mask = (data_movie_1.user_id.isin(common_users))\n",
    "data_movie_1 = data_movie_1[mask]\n",
    "print(data_movie_1[['user_id','rating']].head(), '\\n')\n",
    "\n",
    "mask = (data_movie_2.user_id.isin(common_users))\n",
    "data_movie_2 = data_movie_2[mask]\n",
    "print(data_movie_2[['user_id','rating']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Item-Item similarity functions:\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Returns a distance-based similarity score for item 1 and item 2:\n",
    "def SimEuclid(DataFrame, Item1, Item2, min_common_users=1):\n",
    "    # GET MOVIES OF ITEM1\n",
    "    movies_item1=DataFrame[DataFrame['movie_id'] ==Item1 ]\n",
    "    # GET MOVIES OF ITEM2\n",
    "    movies_item2=DataFrame[DataFrame['movie_id'] ==Item2 ]\n",
    "    \n",
    "    # FIND SHARED USERS\n",
    "    rep=pd.merge(movies_item1, movies_item2, on='user_id')  \n",
    "    \n",
    "    if len(rep)==0:\n",
    "        return 0\n",
    "    if(len(rep)<min_common_users):\n",
    "        return 0\n",
    "    #return distEuclid(rep['rating_x'],rep['rating_y']) \n",
    "    return 1.0/(1.0+euclidean(rep['rating_x'],rep['rating_y'])) \n",
    "\n",
    "# Returns a pearsonCorrealation-based similarity score for item 1 and item 2:\n",
    "def SimPearson(DataFrame, Item1, Item2, min_common_users=1):\n",
    "    # GET MOVIES OF ITEM1\n",
    "    movies_item1=DataFrame[DataFrame['movie_id'] ==Item1 ]\n",
    "    # GET MOVIES OF ITEM2\n",
    "    movies_item2=DataFrame[DataFrame['movie_id'] ==Item2 ]\n",
    "    \n",
    "    # FIND SHARED USERS\n",
    "    rep=pd.merge(movies_item1, movies_item2, on='user_id') \n",
    "    \n",
    "    if len(rep)==0:\n",
    "        return 0    \n",
    "    if(len(rep)<min_common_users):\n",
    "        return 0    \n",
    "    res=pearsonr(rep['rating_x'],rep['rating_y'])[0]\n",
    "    if(isnan(res)):\n",
    "        return 0\n",
    "    return res\n",
    "\n",
    "# Returns a corrected pearsonCorrealation-based similarity score for item 1 and item 2:\n",
    "def SimPearson_Corrected(DataFrame, Item1, Item2, min_common_users=1, pref_common_users=10):\n",
    "    # GET MOVIES OF ITEM1\n",
    "    movies_item1=DataFrame[DataFrame['movie_id'] ==Item1 ]\n",
    "    # GET MOVIES OF ITEM2\n",
    "    movies_item2=DataFrame[DataFrame['movie_id'] ==Item2 ]\n",
    "    \n",
    "    # FIND SHARED USERS\n",
    "    rep=pd.merge(movies_item1, movies_item2, on='user_id')  \n",
    "    \n",
    "    if len(rep)==0:\n",
    "        return 0    \n",
    "    if(len(rep)<min_common_users):\n",
    "        return 0\n",
    "    \n",
    "    res=pearsonr(rep['rating_x'],rep['rating_y'])[0] * min(pref_common_users, len(rep))/pref_common_users\n",
    "    if(isnan(res)):\n",
    "        return 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Object Oriented Programming to:\n",
    "    - compute the Item-Item similarity matrix for all items, using one of similarity functions defined above,\n",
    "    - predict the User rating on a new Item, using all Item-Item similarities. \"\"\" \n",
    "\n",
    "class itemBased_CF:\n",
    "    \"\"\" Collaborative filtering using a custom sim(u,u'). \"\"\"\n",
    "    \n",
    "    def __init__(self, DataFrame, similarity=SimPearson, min_common_users=10, max_sim_items=10):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.sim_method=similarity # Gets recommendations for a item by using a weighted average\n",
    "        self.df=DataFrame\n",
    "        self.sim = pd.DataFrame(np.sum([0]), columns=data_train.movie_id.unique(), index=data_train.movie_id.unique())\n",
    "        self.min_common_users=min_common_users\n",
    "        self.max_sim_items=max_sim_items\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Prepare data structures for estimation. compute Item-Item Similarity Matrix for all items \"\"\"\n",
    "        allItems=set(self.df['movie_id'])\n",
    "        self.sim = {}\n",
    "        for item1 in allItems:\n",
    "            self.sim.setdefault(item1, {})\n",
    "            a=data_train[data_train['movie_id']==item1][['user_id']]\n",
    "            data_reduced=pd.merge(data_train, a, on='user_id')\n",
    "            \n",
    "            for item2 in allItems:\n",
    "                # don't compare the item with itself\n",
    "                if item1==item2: \n",
    "                    continue\n",
    "                self.sim.setdefault(item2, {})\n",
    "                \n",
    "                if (item1 in self.sim[item2]):\n",
    "                    continue  # since correlation matrix is a symmetric matrix\n",
    "                \n",
    "                sim=self.sim_method(data_reduced, item1, item2, self.min_common_users)\n",
    "                \n",
    "                # print(item1, item2, sim)\n",
    "                \n",
    "                if(sim<0):\n",
    "                    self.sim[item1][item2]=0\n",
    "                    self.sim[item2][item1]=0\n",
    "                else:\n",
    "                    self.sim[item1][item2]=sim\n",
    "                    self.sim[item2][item1]=sim\n",
    "                    \n",
    "        self.mean_ratings=mean_rating \\\n",
    "                         =data_train[['user_id','movie_id','rating']].groupby('movie_id')['rating'].mean()\n",
    "                \n",
    "                \n",
    "    def estimate(self, user_id, movie_id):\n",
    "        \n",
    "        totals={}\n",
    "        movie_users=self.df[self.df['user_id'] ==user_id]\n",
    "        rating_num=0.0\n",
    "        rating_den=0.0\n",
    "        allItems=set(movie_users['movie_id'])\n",
    "        listOrdered=sorted([(self.sim[movie_id][other],other) for other in allItems if movie_id!=other], reverse=True)\n",
    "        \n",
    "        for user in range(min(len(listOrdered), self.max_sim_items)):\n",
    "            other=listOrdered[user][1]\n",
    "            rating_num += self.sim[movie_id][other] * (float(movie_users[movie_users['movie_id']==other]['rating'])) #-self.mean_ratings[other]\n",
    "            rating_den += self.sim[movie_id][other]\n",
    "        if rating_den==0: \n",
    "            if self.df.rating[self.df['user_id']==user_id].mean()>0:\n",
    "                # return the mean user rating if there is no similar for the computation\n",
    "                return self.df.rating[self.df['user_id']==user_id].mean()\n",
    "            else:\n",
    "                # else return mean item rating \n",
    "                return self.df.rating[self.df['movie_id']==movie_id].mean()\n",
    "        return rating_num/rating_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate RMSE\n",
    "def  compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to evaluate the Recommender System\n",
    "def  evaluate (estimate_f, data_train, data_test):\n",
    "    \"\"\" RMSE-based predictive performance evaluation. \"\"\"\n",
    "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
    "    estimated = np.array([estimate_f(u,i) if u in data_train.movie_id else 3 for (u,i) in ids_to_estimate ])\n",
    "    \n",
    "    real = data_test.rating.values\n",
    "    return compute_rmse(estimated, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "count=data.groupby('movie_id').count()\n",
    "selectedData=data[data['movie_id'].isin(list(count[count['user_id']>100].reset_index()['movie_id']))]\n",
    "\n",
    "data_train, data_test = train_test_split(selectedData, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "record=itemBased_CF(data_train, similarity=SimEuclid, min_common_users=5, max_sim_items=10)\n",
    "record.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Collaborative Recomender (using Item-Item Euclidian Similarity):  1.134673114522238\n"
     ]
    }
   ],
   "source": [
    "print('RMSE for Collaborative Recomender (using Item-Item Euclidian Similarity):  %s' % \n",
    "                                                                    evaluate(record.estimate, data_train, data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_new=itemBased_CF(data_train, similarity=SimPearson_Corrected, min_common_users=5, max_sim_items=10)\n",
    "record_new.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Collaborative Recomender (using Corrected Item-Item Correlation Similarity):  1.11367658172128\n"
     ]
    }
   ],
   "source": [
    "print('RMSE for Collaborative Recomender (using Corrected Item-Item Correlation Similarity):  %s' % \n",
    "                                                                    evaluate(record_new.estimate, data_train, data_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
